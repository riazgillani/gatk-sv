##########################################################################################

## Base script:   https://portal.firecloud.org/#methods/Talkowski-SV/02_rdtest/10/wdl

## Github commit: talkowski-lab/gatk-sv-v1:<ENTER HASH HERE IN FIRECLOUD>

##########################################################################################

version 1.0

import "GermlineCNVCohort.wdl" as gcnv
import "CollectCoverage.wdl" as cov

workflow GATKSVJoinSamples {
  input {
    Array[File] vcfs
    String batch
    Array[String] samples
    Array[File] counts
    Array[File] cnmops_files
    Array[File] large_gcnv_interval_vcfs

    File sample_coverage_file

    File sr_file
    File pe_file
    File sample_coverage_file
    File exclude_intervals
    File ploidy_calls_tar

    Int? small_cnv_size
    Int? small_cnv_padding

    # Condense read counts
    Int condense_num_bins = 2
    Int condense_bin_size = 200

    File ref_fasta_dict
    File ref_fasta_fai
    File ref_fasta
    String linux_docker
    String sv_base_mini_docker
    String sv_pipeline_docker
    String sv_pipeline_base_docker
    String gcnv_gatk_docker
    String gatk_docker
    File condense_counts_docker

    RuntimeAttr? runtime_attr_merge
    RuntimeAttr? runtime_attr_small_intervals
    RuntimeAttr? runtime_attr_intersect_intervals
    RuntimeAttr? runtime_attr_counts_to_intervals
    RuntimeAttr? runtime_attr_cluster
    RuntimeAttr? runtime_attr_posteriors
    RuntimeAttr? runtime_attr_merge_vcfs
    RuntimeAttr? runtime_attr_condense_counts

    RuntimeAttr? runtime_attr_annotate
    RuntimeAttr? runtime_attr_filter
    RuntimeAttr? runtime_attr_scatter
    RuntimeAttr? runtime_attr_ploidy
    RuntimeAttr? runtime_attr_cohort
    RuntimeAttr? runtime_attr_bundle
    RuntimeAttr? runtime_attr_postprocess
    RuntimeAttr? runtime_attr_explode

    ##################################
    #### required gcnv arguments ####
    ##################################
    File contig_ploidy_priors
    Int num_intervals_per_scatter

    ##################################
    #### optional basic arguments ####
    ##################################
    # If true, AnnotateIntervals will be run to create GC annotations and explicit
    # GC correction will be performed by the model generated by
    Boolean? do_explicit_gc_correction
    File? gatk4_jar_override

    ##################################################
    #### optional arguments for AnnotateIntervals ####
    ##################################################
    File? mappability_track_bed
    File? mappability_track_bed_idx
    File? segmental_duplication_track_bed
    File? segmental_duplication_track_bed_idx
    Int? feature_query_lookahead

    #################################################
    #### optional arguments for FilterIntervals ####
    ################################################
    Boolean? filter_intervals
    File? exclude_intervals_for_filter_intervals_ploidy
    File? exclude_intervals_for_filter_intervals_cnv
    Float? minimum_gc_content
    Float? maximum_gc_content
    Float? minimum_mappability
    Float? maximum_mappability
    Float? minimum_segmental_duplication_content
    Float? maximum_segmental_duplication_content
    Int? low_count_filter_count_threshold
    Float? low_count_filter_percentage_of_samples
    Float? extreme_count_filter_minimum_percentile
    Float? extreme_count_filter_maximum_percentile
    Float? extreme_count_filter_percentage_of_samples

    ########################################################################
    #### optional arguments for DetermineGermlineContigPloidyCohortMode ####
    ########################################################################
    Float? ploidy_mean_bias_standard_deviation
    Float? ploidy_mapping_error_rate
    Float? ploidy_global_psi_scale
    Float? ploidy_sample_psi_scale

    ############################################################
    #### optional arguments for GermlineCNVCallerCohortMode ####
    ############################################################
    Float? gcnv_p_alt
    Float? gcnv_p_active
    Float? gcnv_cnv_coherence_length
    Float? gcnv_class_coherence_length
    Int? gcnv_max_copy_number

    # optional arguments for germline CNV denoising model
    Int? gcnv_max_bias_factors
    Float? gcnv_mapping_error_rate
    Float? gcnv_interval_psi_scale
    Float? gcnv_sample_psi_scale
    Float? gcnv_depth_correction_tau
    Float? gcnv_log_mean_bias_standard_deviation
    Float? gcnv_init_ard_rel_unexplained_variance
    Int? gcnv_num_gc_bins
    Float? gcnv_gc_curve_standard_deviation
    String? gcnv_copy_number_posterior_expectation_mode
    Boolean? gcnv_enable_bias_factors
    Int? gcnv_active_class_padding_hybrid_mode

    # optional arguments for Hybrid ADVI
    Float? gcnv_learning_rate
    Float? gcnv_adamax_beta_1
    Float? gcnv_adamax_beta_2
    Int? gcnv_log_emission_samples_per_round
    Float? gcnv_log_emission_sampling_median_rel_error
    Int? gcnv_log_emission_sampling_rounds
    Int? gcnv_max_advi_iter_first_epoch
    Int? gcnv_max_advi_iter_subsequent_epochs
    Int? gcnv_min_training_epochs
    Int? gcnv_max_training_epochs
    Float? gcnv_initial_temperature
    Int? gcnv_num_thermal_advi_iters
    Int? gcnv_convergence_snr_averaging_window
    Float? gcnv_convergence_snr_trigger_threshold
    Int? gcnv_convergence_snr_countdown_window
    Int? gcnv_max_calling_iters
    Float? gcnv_caller_update_convergence_threshold
    Float? gcnv_caller_internal_admixing_rate
    Float? gcnv_caller_external_admixing_rate
    Boolean? gcnv_disable_annealing

    ###################################################
    #### arguments for PostprocessGermlineCNVCalls ####
    ###################################################
    Int ref_copy_number_autosomal_contigs
    Array[String]? allosomal_contigs
  }

  scatter (i in range(length(vcfs))) {
    File vcf_indexes_ = vcfs[i] + ".tbi"
  }

  scatter (i in range(length(cnmops_files))) {
    File cnmops_file_indexes_ = cnmops_files[i] + ".tbi"
  }

  scatter (i in range(length(large_gcnv_interval_vcfs))) {
    File large_gcnv_interval_vcf_indexes_ = large_gcnv_interval_vcfs[i] + ".tbi"
  }

  File sr_index_ = sr_file + ".tbi"
  File pe_index_ = pe_file + ".tbi"
  File exclude_intervals_index_ = exclude_intervals + ".tbi"

  call MergeSVCalls {
    input:
      vcfs = vcfs,
      vcf_indexes = vcf_indexes_,
      cnmops_files = cnmops_files,
      cnmops_file_indexes = cnmops_file_indexes_,
      batch = batch,
      ref_fasta_dict = ref_fasta_dict,
      gatk_docker = gatk_docker,
      runtime_attr_override = runtime_attr_merge
  }

  # TODO: shard VCF using single-linkage clustering (not single-linked => not max-clique)
  # TODO: single-linkage clustering can be sharded by chromosome arm

  call ClusterVariants {
    input:
      calls_file = MergeSVCalls.out,
      calls_file_index = MergeSVCalls.out_index,
      sr_file = sr_file,
      sr_index = sr_index_,
      pe_file = pe_file,
      pe_index = pe_index_,
      sample_coverage_file = sample_coverage_file,
      exclude_intervals = exclude_intervals,
      exclude_intervals_index = exclude_intervals_index_,
      ref_fasta_dict = ref_fasta_dict,
      batch = batch,
      gatk_docker = gatk_docker,
      runtime_attr_override = runtime_attr_cluster
  }

  scatter (i in range(length(samples))) {
    call cov.CondenseReadCounts as CondenseReadCounts {
      input:
        counts = counts[i],
        sample = samples[i],
        num_bins = condense_num_bins,
        expected_bin_size = condense_bin_size,
        condense_counts_docker = condense_counts_docker,
        runtime_attr_override = runtime_attr_condense_counts
    }
  }

  call SmallCNVIntervals {
    input:
      vcf = ClusterVariants.out,
      vcf_index = ClusterVariants.out_index,
      ref_fasta_fai = ref_fasta_fai,
      size = small_cnv_size,
      padding = small_cnv_padding,
      batch = batch,
      sv_pipeline_docker = sv_pipeline_docker,
      runtime_attr_override = runtime_attr_small_intervals
  }

  call IntersectCountsWithIntervals {
    input:
      counts = CondenseReadCounts.out[0],
      interval_list = SmallCNVIntervals.out,
      output_name = "~{batch}.small_cnvs.counts.tsv",
      gzip = true,
      sv_base_mini_docker = sv_base_mini_docker,
      runtime_attr_override = runtime_attr_intersect_intervals
  }

  call cov.CountsToIntervals {
    input:
      counts = IntersectCountsWithIntervals.out,
      output_name = "small_cnv_intervals",
      linux_docker = linux_docker,
      runtime_attr_override = runtime_attr_counts_to_intervals
  }

  call gcnv.CNVGermlineCohortWorkflow {
    input:
      preprocessed_intervals = CountsToIntervals.out,
      filter_intervals = filter_intervals,
      counts = CondenseReadCounts.out,
      count_entity_ids = samples,
      cohort_entity_id = batch,
      contig_ploidy_priors = contig_ploidy_priors,
      num_intervals_per_scatter = num_intervals_per_scatter,
      ref_fasta_dict = ref_fasta_dict,
      ref_fasta_fai = ref_fasta_fai,
      ref_fasta = ref_fasta,
      exclude_intervals_for_filter_intervals_ploidy=exclude_intervals_for_filter_intervals_ploidy,
      exclude_intervals_for_filter_intervals_cnv=exclude_intervals_for_filter_intervals_cnv,
      do_explicit_gc_correction = do_explicit_gc_correction,
      gcnv_enable_bias_factors = gcnv_enable_bias_factors,
      ref_copy_number_autosomal_contigs = ref_copy_number_autosomal_contigs,
      allosomal_contigs = allosomal_contigs,
      sv_base_mini_docker = sv_base_mini_docker,
      gatk_docker = gcnv_gatk_docker,
      linux_docker = linux_docker,
      gcnv_learning_rate = gcnv_learning_rate,
      gcnv_max_advi_iter_first_epoch = gcnv_max_advi_iter_first_epoch,
      gcnv_num_thermal_advi_iters = gcnv_num_thermal_advi_iters,
      gcnv_max_advi_iter_subsequent_epochs = gcnv_max_advi_iter_subsequent_epochs,
      gcnv_max_training_epochs = gcnv_max_training_epochs,
      gcnv_min_training_epochs = gcnv_min_training_epochs,
      gcnv_convergence_snr_averaging_window = gcnv_convergence_snr_averaging_window,
      gcnv_convergence_snr_countdown_window = gcnv_convergence_snr_countdown_window,
      gcnv_cnv_coherence_length = gcnv_cnv_coherence_length,
      gcnv_class_coherence_length = gcnv_class_coherence_length,
      gcnv_copy_number_posterior_expectation_mode = gcnv_copy_number_posterior_expectation_mode,
      gcnv_log_emission_sampling_rounds = gcnv_log_emission_sampling_rounds,
      gcnv_p_alt = gcnv_p_alt,
      gcnv_sample_psi_scale = gcnv_sample_psi_scale,
      ploidy_sample_psi_scale = ploidy_sample_psi_scale,
      gcnv_caller_update_convergence_threshold = gcnv_caller_update_convergence_threshold,
      gcnv_convergence_snr_trigger_threshold = gcnv_convergence_snr_trigger_threshold,
      gcnv_interval_psi_scale = gcnv_interval_psi_scale,
      gcnv_log_emission_sampling_median_rel_error = gcnv_log_emission_sampling_median_rel_error,
      gcnv_log_mean_bias_standard_deviation = gcnv_log_mean_bias_standard_deviation,
      gcnv_max_bias_factors = gcnv_max_bias_factors,
      gcnv_max_calling_iters = gcnv_max_calling_iters,
      ploidy_global_psi_scale = ploidy_global_psi_scale,
      ploidy_mean_bias_standard_deviation = ploidy_mean_bias_standard_deviation,
      gcnv_depth_correction_tau = gcnv_depth_correction_tau,
      runtime_attr_annotate = runtime_attr_annotate,
      runtime_attr_filter = runtime_attr_filter,
      runtime_attr_scatter = runtime_attr_scatter,
      runtime_attr_ploidy = runtime_attr_ploidy,
      runtime_attr_cohort = runtime_attr_cohort,
      runtime_attr_bundle = runtime_attr_bundle,
      runtime_attr_postprocess = runtime_attr_postprocess,
      runtime_attr_explode = runtime_attr_explode
  }

  call MergeVcfs as MergeSmallCNVVcfs {
    input:
      vcfs = CNVGermlineCohortWorkflow.genotyped_intervals_vcfs,
      vcf_indexes = CNVGermlineCohortWorkflow.genotyped_intervals_vcf_indexes,
      output_name = "~{batch}.gcnv_small_intervals.vcf.gz",
      sv_base_mini_docker = sv_base_mini_docker,
      runtime_attr_override = runtime_attr_merge_vcfs
  }

  call MergeVcfs as MergeLargeCNVVcfs {
    input:
      vcfs = large_gcnv_interval_vcfs,
      vcf_indexes = large_gcnv_interval_vcf_indexes_,
      output_name = "~{batch}.gcnv_large_intervals.vcf.gz",
      sv_base_mini_docker = sv_base_mini_docker,
      runtime_attr_override = runtime_attr_merge_vcfs
  }

  call CopyNumberPosteriors {
    input:
      vcf = ClusterVariants.out,
      vcf_index = ClusterVariants.out_index,
      gcnv_intervals_vcfs = [MergeLargeCNVVcfs.out, MergeSmallCNVVcfs.out],
      gcnv_intervals_vcf_indexes = [MergeLargeCNVVcfs.out_index, MergeSmallCNVVcfs.out_index],
      ploidy_calls_tar = ploidy_calls_tar,
      ref_fasta_dict = ref_fasta_dict,
      batch = batch,
      gatk_docker = gatk_docker,
      runtime_attr_override = runtime_attr_posteriors
  }

  output {
    File out = CopyNumberPosteriors.out
    File out_index = CopyNumberPosteriors.out_index
  }
}

task CopyNumberPosteriors {
  input {
    File vcf
    File vcf_index
    Array[File] gcnv_intervals_vcfs
    Array[File] gcnv_intervals_vcf_indexes
    File ploidy_calls_tar
    File ref_fasta_dict
    String batch
    String gatk_path = "/gatk/gatk"
    String gatk_docker
    RuntimeAttr? runtime_attr_override
  }

  RuntimeAttr default_attr = object {
    cpu_cores: 1,
    mem_gb: 7.5,
    disk_gb: 10,
    boot_disk_gb: 10,
    preemptible_tries: 3,
    max_retries: 1
  }
  RuntimeAttr runtime_attr = select_first([runtime_attr_override, default_attr])

  Float mem_gb = select_first([runtime_attr.mem_gb, default_attr.mem_gb])
  Int java_mem_mb = ceil(mem_gb * 1000 * 0.8)

  output {
    File out = "~{batch}.aggregated.vcf.gz"
    File out_index = "~{batch}.aggregated.vcf.gz.tbi"
  }
  command <<<
    set -euo pipefail

    mkdir ploidy-calls
    tar xzf ~{ploidy_calls_tar} -C ploidy-calls
    ls ploidy-calls/SAMPLE_*/contig_ploidy.tsv > ploidy_files.list

    # Create arguments file
    echo "--cnv-intervals-vcf ~{sep=" --cnv-intervals-vcf " gcnv_intervals_vcfs}" > args.txt
    while read line; do
      echo "--ploidy-calls-file $line" >> args.txt
    done < ploidy_files.list

    ~{gatk_path} --java-options "-Xmx~{java_mem_mb}m" SVCopyNumberPosteriors \
      --arguments_file args.txt \
      --variant ~{vcf} \
      --output ~{batch}.aggregated.vcf.gz \
      --sequence-dictionary ~{ref_fasta_dict} \
      --genotype-depth-calls

  >>>
  runtime {
    cpu: select_first([runtime_attr.cpu_cores, default_attr.cpu_cores])
    memory: mem_gb + " GiB"
    disks: "local-disk " + select_first([runtime_attr.disk_gb, default_attr.disk_gb]) + " HDD"
    bootDiskSizeGb: select_first([runtime_attr.boot_disk_gb, default_attr.boot_disk_gb])
    docker: gatk_docker
    preemptible: select_first([runtime_attr.preemptible_tries, default_attr.preemptible_tries])
    maxRetries: select_first([runtime_attr.max_retries, default_attr.max_retries])
  }
}

task ClusterVariants {
  input {
    File calls_file
    File calls_file_index
    File sr_file
    File sr_index
    File pe_file
    File pe_index
    File sample_coverage_file
    File exclude_intervals
    File exclude_intervals_index
    File ref_fasta_dict
    String batch
    String gatk_path = "/gatk/gatk"
    String gatk_docker
    RuntimeAttr? runtime_attr_override
  }

  RuntimeAttr default_attr = object {
    cpu_cores: 1,
    mem_gb: 7.5,
    disk_gb: 100,
    boot_disk_gb: 10,
    preemptible_tries: 3,
    max_retries: 1
  }
  RuntimeAttr runtime_attr = select_first([runtime_attr_override, default_attr])

  Float mem_gb = select_first([runtime_attr.mem_gb, default_attr.mem_gb])
  Int java_mem_mb = ceil(mem_gb * 1000 * 0.8)

  output {
    File out = "~{batch}.clustered.vcf.gz"
    File out_index = "~{batch}.clustered.vcf.gz.tbi"
  }
  command <<<
    set -euo pipefail
    ~{gatk_path} --java-options "-Xmx~{java_mem_mb}m" SVCluster \
      -V ~{calls_file} \
      -O ~{batch}.clustered.vcf.gz \
      -XL ~{exclude_intervals} \
      --sequence-dictionary ~{ref_fasta_dict} \
      --split-reads-file ~{sr_file} \
      --discordant-pairs-file ~{pe_file} \
      --sample-coverage ~{sample_coverage_file}
  >>>
  runtime {
    cpu: select_first([runtime_attr.cpu_cores, default_attr.cpu_cores])
    memory: mem_gb + " GiB"
    disks: "local-disk " + select_first([runtime_attr.disk_gb, default_attr.disk_gb]) + " HDD"
    bootDiskSizeGb: select_first([runtime_attr.boot_disk_gb, default_attr.boot_disk_gb])
    docker: gatk_docker
    preemptible: select_first([runtime_attr.preemptible_tries, default_attr.preemptible_tries])
    maxRetries: select_first([runtime_attr.max_retries, default_attr.max_retries])
  }
}

task MergeSVCalls {
  input {
    Array[File] vcfs
    Array[File] vcf_indexes
    Array[File] cnmops_files
    Array[File] cnmops_file_indexes
    File ref_fasta_dict
    String batch
    String gatk_path = "/gatk/gatk"
    String gatk_docker
    RuntimeAttr? runtime_attr_override
  }

  RuntimeAttr default_attr = object {
    cpu_cores: 1,
    mem_gb: 3.75,
    disk_gb: 10,
    boot_disk_gb: 10,
    preemptible_tries: 3,
    max_retries: 1
  }
  RuntimeAttr runtime_attr = select_first([runtime_attr_override, default_attr])

  Float mem_gb = select_first([runtime_attr.mem_gb, default_attr.mem_gb])
  Int java_mem_mb = ceil(mem_gb * 1000 * 0.8)

  output {
    File out = "~{batch}.merged.sv.tsv.gz"
    File out_index = "~{batch}.merged.sv.tsv.gz.tbi"
  }
  command <<<
    set -euo pipefail

    # Create arguments file
    touch args.txt
    while read line; do
      echo "--cnmops $line" >> args.txt
    done < ~{write_lines(cnmops_files)}

    while read line; do
      echo "-V $line" >> args.txt
    done < ~{write_lines(vcfs)}

    ~{gatk_path} --java-options "-Xmx~{java_mem_mb}m" MergeSVCalls \
      --arguments_file args.txt \
      --sequence-dictionary ~{ref_fasta_dict} \
      --output ~{batch}.merged.sv.tsv.gz \
      --ignore-dict
  >>>
  runtime {
    cpu: select_first([runtime_attr.cpu_cores, default_attr.cpu_cores])
    memory: mem_gb + " GiB"
    disks: "local-disk " + select_first([runtime_attr.disk_gb, default_attr.disk_gb]) + " HDD"
    bootDiskSizeGb: select_first([runtime_attr.boot_disk_gb, default_attr.boot_disk_gb])
    docker: gatk_docker
    preemptible: select_first([runtime_attr.preemptible_tries, default_attr.preemptible_tries])
    maxRetries: select_first([runtime_attr.max_retries, default_attr.max_retries])
  }
}

task MergeVcfs {
  input {
    Array[File] vcfs
    Array[File] vcf_indexes
    String output_name
    String sv_base_mini_docker
    RuntimeAttr? runtime_attr_override
  }

  RuntimeAttr default_attr = object {
    cpu_cores: 1,
    mem_gb: 3.75,
    disk_gb: 10,
    boot_disk_gb: 10,
    preemptible_tries: 3,
    max_retries: 1
  }
  RuntimeAttr runtime_attr = select_first([runtime_attr_override, default_attr])

  output {
    File out = "~{output_name}"
    File out_index = "~{output_name}.tbi"
  }
  command <<<

    set -euo pipefail
    bcftools merge --file-list ~{write_lines(vcfs)} -o ~{output_name} --output-type z
    tabix ~{output_name}

  >>>
  runtime {
    cpu: select_first([runtime_attr.cpu_cores, default_attr.cpu_cores])
    memory: select_first([runtime_attr.mem_gb, default_attr.mem_gb]) + " GiB"
    disks: "local-disk " + select_first([runtime_attr.disk_gb, default_attr.disk_gb]) + " HDD"
    bootDiskSizeGb: select_first([runtime_attr.boot_disk_gb, default_attr.boot_disk_gb])
    docker: sv_base_mini_docker
    preemptible: select_first([runtime_attr.preemptible_tries, default_attr.preemptible_tries])
    maxRetries: select_first([runtime_attr.max_retries, default_attr.max_retries])
  }
}

task SmallCNVIntervals {
  input {
    File vcf
    File vcf_index
    File ref_fasta_fai
    Int size = 5000
    Int padding = 1000
    String batch
    String sv_pipeline_docker
    RuntimeAttr? runtime_attr_override
  }

  RuntimeAttr default_attr = object {
    cpu_cores: 1,
    mem_gb: 1.0,
    disk_gb: 10,
    boot_disk_gb: 10,
    preemptible_tries: 3,
    max_retries: 1
  }
  RuntimeAttr runtime_attr = select_first([runtime_attr_override, default_attr])

  output {
    File out = "~{batch}.small_cnv_intervals.bed"
  }
  command <<<
    set -euo pipefail
    python > small_cnvs.bed <<EOF
import sys
from pysam import VariantFile
vcf = VariantFile('~{vcf}')
types = set(['DEL', 'DUP', 'BND'])
for record in vcf.fetch():
  if record.info['SVTYPE'] in types \
    and record.info['ALGORITHMS'] != 'depth' \
    and record.chrom == record.info['CHR2'] \
    and record.info['STRANDS'][0] != record.info['STRANDS'][1] \
    and record.stop - record.pos < ~{size}:
    fields = [record.chrom, str(record.pos), str(record.stop)]
    print('\t'.join(fields))
EOF

    bedtools slop -b ~{padding} -i small_cnvs.bed -g ~{ref_fasta_fai} \
      | bedtools merge \
      > ~{batch}.small_cnv_intervals.bed

  >>>
  runtime {
    cpu: select_first([runtime_attr.cpu_cores, default_attr.cpu_cores])
    memory: select_first([runtime_attr.mem_gb, default_attr.mem_gb]) + " GiB"
    disks: "local-disk " + select_first([runtime_attr.disk_gb, default_attr.disk_gb]) + " HDD"
    bootDiskSizeGb: select_first([runtime_attr.boot_disk_gb, default_attr.boot_disk_gb])
    docker: sv_pipeline_docker
    preemptible: select_first([runtime_attr.preemptible_tries, default_attr.preemptible_tries])
    maxRetries: select_first([runtime_attr.max_retries, default_attr.max_retries])
  }
}

task IntersectCountsWithIntervals {
  input {
    File counts
    File interval_list
    String output_name
    Boolean gzip = true
    String sv_base_mini_docker
    RuntimeAttr? runtime_attr_override
  }

  RuntimeAttr default_attr = object {
    cpu_cores: 1,
    mem_gb: 3.75,
    disk_gb: 10,
    boot_disk_gb: 10,
    preemptible_tries: 3,
    max_retries: 1
  }
  RuntimeAttr runtime_attr = select_first([runtime_attr_override, default_attr])

  output {
    File out = "~{output_name}.gz"
  }
  command <<<

    set -euo pipefail
    zgrep -B9999999999 -m1 -v "^@" ~{counts} > ~{output_name}
    zgrep -v "^@" ~{counts} | tail -n +2 | bedtools intersect -wa -sorted -u -a stdin -b ~{interval_list} >> ~{output_name}
    if ~{gzip}; then
      bgzip ~{output_name}
    fi

  >>>
  runtime {
    cpu: select_first([runtime_attr.cpu_cores, default_attr.cpu_cores])
    memory: select_first([runtime_attr.mem_gb, default_attr.mem_gb]) + " GiB"
    disks: "local-disk " + select_first([runtime_attr.disk_gb, default_attr.disk_gb]) + " HDD"
    bootDiskSizeGb: select_first([runtime_attr.boot_disk_gb, default_attr.boot_disk_gb])
    docker: sv_base_mini_docker
    preemptible: select_first([runtime_attr.preemptible_tries, default_attr.preemptible_tries])
    maxRetries: select_first([runtime_attr.max_retries, default_attr.max_retries])
  }
}

task ShardVcf {
  input {
    File pesr_vcf
    Int records_per_shard
    String sv_pipeline_base_docker
    RuntimeAttr? runtime_attr_override
  }

  RuntimeAttr default_attr = object {
    cpu_cores: 1,
    mem_gb: 3.75,
    disk_gb: 10,
    boot_disk_gb: 10,
    preemptible_tries: 3,
    max_retries: 1
  }
  RuntimeAttr runtime_attr = select_first([runtime_attr_override, default_attr])

  Array[String] svtypes = ["DEL", "DUP", "INV", "INS"]
  Array[String] bnd_strands = ["++", "+-", "-+", "--"]
  Array[String] bnd_strand_labels = ["plus_plus", "plus_minus", "minus_plus", "minus_minus"]

  output {
    Array[File] out = glob("*.shard_*.vcf.gz")
  }
  command <<<

    set -euo pipefail
    sgrep() { grep "$@" || test $? = 1; }
    NAME=$(basename ~{pesr_vcf} .vcf.gz)
    SVTYPES=(~{sep=" " svtypes})
    zcat ~{pesr_vcf} | grep ^# > header
    for svtype in ${SVTYPES[@]}; do
      zcat ~{pesr_vcf} | grep -v ^# | sgrep -w "<${svtype}>" > records
      NUM_RECORDS=$(cat records | wc -l)
      if [ "${NUM_RECORDS}" -gt "0" ]; then
        CHUNKS=$(python -c "from math import ceil; print(ceil($NUM_RECORDS/float(~{records_per_shard})))")
        split -a9 -d -n l/$CHUNKS records records.shard_
        i=0
        for chunk in records.shard_*; do
          padded=`printf %09d $i`
          cat header $chunk | bgzip -c > $NAME.${svtype}.shard_${padded}.vcf.gz
          i=$((i+1))
        done
        rm records records.shard_*
      else
        echo "No records of type ${svtype} found"
      fi
    done

    # Handle BNDs separately
    STRANDS=(~{sep=" " bnd_strands})
    STRAND_STR=(~{sep=" " bnd_strand_labels})
    svtype="BND"
    num_strand_types=${#STRANDS[*]}
    for (( j=0; j<=$(( $num_strand_types -1 )); j++ )); do
      strand=${STRANDS[$j]}
      strand_str=${STRAND_STR[$j]}
      zcat ~{pesr_vcf} | grep -v ^# | sgrep -w "<${svtype}>" | sgrep "STRANDS=${strand}" > records
      NUM_RECORDS=$(cat records | wc -l)
      if [ "${NUM_RECORDS}" -gt "0" ]; then
        CHUNKS=$(python -c "from math import ceil; print(ceil($NUM_RECORDS/float(~{records_per_shard})))")
        split -a9 -d -n l/$CHUNKS records records.shard_
        i=0
        for chunk in records.shard_*; do
          padded=`printf %09d $i`
          cat header $chunk | bgzip -c > $NAME.${svtype}.${strand_str}.shard_${padded}.vcf.gz
          i=$((i+1))
        done
        rm records records.shard_*
      else
        echo "No records of type ${svtype} found"
      fi
    done

  >>>
  runtime {
    cpu: select_first([runtime_attr.cpu_cores, default_attr.cpu_cores])
    memory: select_first([runtime_attr.mem_gb, default_attr.mem_gb]) + " GiB"
    disks: "local-disk " + select_first([runtime_attr.disk_gb, default_attr.disk_gb]) + " HDD"
    bootDiskSizeGb: select_first([runtime_attr.boot_disk_gb, default_attr.boot_disk_gb])
    docker: sv_pipeline_base_docker
    preemptible: select_first([runtime_attr.preemptible_tries, default_attr.preemptible_tries])
    maxRetries: select_first([runtime_attr.max_retries, default_attr.max_retries])
  }
}
